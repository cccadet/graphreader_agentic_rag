{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data to Neo4j "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from graphreader_agentic_rag.documents import Importer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Importer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Import\n",
    "\n",
    "This example shows how to import text data from Wikipedia and process it using the `Importer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(doc_content_chars_max=10000)\n",
    ")\n",
    "text = wikipedia.run(\"Tetragonisca_angustula\")\n",
    "\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started extraction at: 2024-11-18 22:41:48.643418\n",
      "Total text chunks: 1\n",
      "Finished LLM extraction after: 0:00:28.686690\n",
      "Finished import at: 0:00:30.184770\n"
     ]
    }
   ],
   "source": [
    "await loader.process_document(text=text, document_name=\"Tetragonisca_angustula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single File Import\n",
    "\n",
    "This example shows how to import a single text file and process it using the `Importer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(doc_content_chars_max=10000)\n",
    ")\n",
    "text = wikipedia.run(\"Melipona_quadrifasciata\")\n",
    "\n",
    "path = \"input/text/\"\n",
    "\n",
    "# Save the text to a file\n",
    "with open(os.path.join(path, \"Melipona_quadrifasciata.txt\"), \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Melipona_quadrifasciata\n",
      "Started extraction at: 2024-11-18 22:44:38.466674\n",
      "Total text chunks: 1\n",
      "Finished LLM extraction after: 0:00:13.582610\n",
      "Finished import at: 0:00:14.697455\n"
     ]
    }
   ],
   "source": [
    "await loader.process_single_file(filepath=path, filename=\"Melipona_quadrifasciata.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Import\n",
    "\n",
    "This example shows how to import all text files in a directory and process them using the `Importer` class.\n",
    "\n",
    "You can also specify a separator for each chunk of text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Western_honey_bee\n",
      "Started extraction at: 2024-11-18 23:24:16.576298\n",
      "Total text chunks: 13\n",
      "Processing Stingless_bee\n",
      "Started extraction at: 2024-11-18 23:24:16.577279\n",
      "Total text chunks: 4\n",
      "Finished LLM extraction after: 0:00:19.962224\n",
      "Finished import at: 0:00:21.713879\n",
      "Finished LLM extraction after: 0:00:23.484074\n",
      "Finished import at: 0:00:25.594698\n"
     ]
    }
   ],
   "source": [
    "path = \"input/pages/\"\n",
    "await loader.process_all_files(filepath=path, separator = [\"----------------------\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j_graphreader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
